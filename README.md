# Motore RAG(Retrieval-Augmented Generation)
## 1. Definizione
Un motore AI RAG (o sistema RAG) √® un‚Äôarchitettura che combina modelli di linguaggio (LLM) con un motore di ricerca interno per produrre risposte pi√π accurate, verificabili e basate su dati propri.
## 2. Dipendenze aggiuntive da installare
```bash
composer require \
    smalot/pdfparser \
    phpoffice/phpword \
    openai-php/client \
    partitech/doctrine-pgvector
```
## 3. PostgreSQL + pgvector + Doctrine
### Installare postgres + pgvector
```bash
sudo apt install postgresql-18 postgresql-18-pgvector
```
### Nel database PostgreSQL (una volta sola), necessari permessi di admin
```sql
CREATE EXTENSION IF NOT EXISTS vector;
```
### Creare indice ivfflat per velocizzare le ricerche(dopo aver generato la entity DocumentChunk e relativa tabella su postgres)
```sql
CREATE INDEX IF NOT EXISTS document_chunk_embedding_ivfflat_idx
ON document_chunk
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100)
```
### Cos'√® pgvector
__pgvector__ √® un‚Äôestensione per PostgreSQL che aggiunge:
* un tipo di colonna: vector(N) ‚Üí un array di N numeri (float)
* operatori e funzioni per confrontare questi vettori (distanze, similarit√†)
* indici speciali (ivfflat / hnsw) per rendere le ricerche veloci

Nel nostro schema abbiamno:
```php
#[ORM\Column(type: 'vector', length: 1536)]
private array $embedding;
```
questo campo su `DocumentChunk` √® letteralmente:  
***il posto dove salviamo il vettore di embedding del chunk di testo***
### Definizione di embedding

Quando viene indicizzato un chunk:
* viene preso il testo (__$chunkText__)
* viene passato al modello `text-embedding-3-small` di Ollama
* il modello restituisce un array di 1536 numeri tipo:
```json
[-0.023, 0.114, ..., 0.002]
```
Questo vettore rappresenta il significato del testo in uno spazio numerico.  
In questo spazio:  
Testi ‚Äúsimili‚Äù sono ‚Äúvicini‚Äù; testi diversi sono ‚Äúlontani‚Äù.

`pgvector` serve esattamente a questo:  
Postgres li usa per memorizzare questi vettori e confrontarli.

All'interno dell'applicativo:
* quando indicizzi ‚Üí salvi per ogni `DocumentChunk` il suo `embedding` (vector(1536))
* quando interroghi il chatbot ‚Üí calcoli l‚Äô`embedding` della domanda e lo confronti con quelli salvati.
### Cos‚Äô√® cosine_similarity e cosa fa nella query
Nel ChatbotService abbiamo:
```php
$qb = $this->em->createQueryBuilder()
    ->select('c', 'f')
    ->from(DocumentChunk::class, 'c')
    ->join('c.file', 'f')
    ->where('c.embedding IS NOT NULL')
    ->orderBy('cosine_similarity(c.embedding, :vec)', 'DESC')
    ->setMaxResults(5)
    ->setParameter('vec', $queryVec);
```
Qui accadono 2 cose molto importanti:
1. `:vec` √® l‚Äôembedding della domanda (array di 1536 float).
2. `cosine_similarity(c.embedding, :vec)` √® una funzione di pgvector che calcola quanto sono simili i due vettori.
### Cos‚Äô√® la cosine similarity in parole povere
Immagina ogni embedding come una freccia in uno spazio a 1536 dimensioni üòÖ

La `cosine similarity` misura l‚Äôangolo tra le due frecce(domanda, chunk):
* angolo piccolo ‚Üí frecce ‚Äúpuntano‚Äù nella stessa direzione ‚Üí `contenuti simili`
* angolo grande ‚Üí frecce ‚Äúpuntano‚Äù in direzioni diverse ‚Üí `contenuti diversi`

Il valore √® tra -1 e 1:
* 1 ‚Üí identici
* 0 ‚Üí non correlati
* -1 ‚Üí opposti

Quando si esegue:
```sql
ORDER BY cosine_similarity(c.embedding, :vec) DESC
```
si sta chiedendo:  
***‚ÄúRecupera per primi i chunk il cui significato √® pi√π vicino al significato della domanda‚Äù.***
### In config/packages/doctrine.yaml aggiungi il tipo e le funzioni DQL:
```yaml
doctrine:
  dbal:
    # ... il config solito (url, ecc.)
    types:
      vector: Partitech\DoctrinePgVector\Type\VectorType

  orm:
    # ...
    dql:
      string_functions:
        cosine_similarity: Partitech\DoctrinePgVector\Query\CosineSimilarity
        distance: Partitech\DoctrinePgVector\Query\Distance
```
## 4. Backend AI Ollama/Open
### Variabili d'ambiente, nel file .env.local
```env
DATABASE_URL="postgresql://app:!ChangeMe!@127.0.0.1:5432/app?serverVersion=16&charset=utf8"

# Ollama
AI_BACKEND=ollama
OLLAMA_HOST=http://localhost:11434
OLLAMA_CHAT_MODEL=llama3.2
OLLAMA_EMBED_MODEL=nomic-embed-text

# OpenAI
#AI_BACKEND=openai
#OPENAI_API_KEY=sk-...
#OPENAI_CHAT_MODEL=gpt-5.1-mini
#OPENAI_EMBED_MODEL=text-embedding-3-small

## RAG Test Mode e Fallback
APP_AI_TEST_MODE=true
APP_AI_OFFLINE_FALLBACK=false

# Postgres pgvector - sonde per indice ivfflat
APP_IVFFLAT_PROBES=10
```
### Configurazione AI_BACKEND, PDF parser, Ivfflat Probes, nel file services.yaml
```yaml
parameters:
  # ...

  ai.backend: '%env(AI_BACKEND)%' # ollama | openai

services:
  # ...

  # abilta il servizio per il parsing dei file PDF
  Smalot\PdfParser\Parser: ~

  # Middleware per abilitare l'uso delle sonde sugli indici ivfflat(pgvector)
  App\Middleware\PgvectorIvfflatMiddleware:
      arguments:
          $probes: '%env(int:APP_IVFFLAT_PROBES)%'

  # AiClientInterface per gestire il backend Ollama/OpenAi
  App\AI\AiClientInterface:
      factory: [ 'App\AI\AiClientFactory', 'create' ]
      arguments: [ '%ai.backend%' ]
```
Tramite la variabile di ambiente `APP_IVFFLAT_PROBES`, impostiamo il rapporto qualit√† velocit√† del nostro sistema RAG:
* 5‚Äì10 = super veloce
* 20‚Äì30 = molto preciso
* 50‚Äì100 = qualit√† altissima (RAG pi√π consistente, pi√π lento)
## 5. Command per indicizzare per open-ai
### Esempi di utilizzo
### 1. Full index, sfruttando hash (solo file nuovi/modificati)
```bash
php bin/console app:index-docs -v
```
### 2. Reindicizza TUTTO ignorando hash
```bash
php bin/console app:index-docs --force-reindex -v
```
### 3. Solo la sotto-cartella manuali/
```bash
php bin/console app:index-docs --path=manuali --path=log/2025 -v
```
### 4. Simulazione pura (solo vedere cosa succederebbe)
```bash
php bin/console app:index-docs --dry-run -v
```
### 5. Indicizzare davvero, ma con embeddings finti (test locale)
```bash
php bin/console app:index-docs --test-mode -v
# oppure: APP_AI_TEST_MODE=true php bin/console app:index-docs -v
```
## 6. Command per vedere l'elenco dei file indicizzati
### Esempi di utilizzo
### 1. Elenco base (max 50):
```bash
php bin/console app:list-docs
```
### 2. Filtra per path (es. solo roba con ‚Äútrast‚Äù nel nome):
```bash
php bin/console app:list-docs --path=trast
```
### 3. Mostra fino a 200 file:
```bash
php bin/console app:list-docs --limit=200
```
### 4. Path + limit insieme:
```bash
php bin/console app:list-docs --path=manuali --limit=20
```
# 7. Command per rimuovere file dell'indice
### Esempi di utilizzo
### 1. Eliminare un singolo file indicizzato
```bash
php bin/console app:unindex-file "manuali/helix.md"
```
### 2. Eliminare tutti i file sotto una cartella
```bash
php bin/console app:unindex-file "^manuali/"
```
### 3. Eliminare tutti i PDF
```bash
php bin/console app:unindex-file "\\.pdf$"
```
### 4. Eliminare TUTTO l‚Äôindice (equivalente a reset totale)
```bash
php bin/console app:unindex-file ".*"
```